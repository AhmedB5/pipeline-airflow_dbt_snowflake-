# Pipeline: Airflow + dbt + Snowflake

## Overview
This project implements a modern data pipeline using:
- **Snowflake** as the cloud data warehouse  
- **Astro + Airflow** for orchestration  
- **dbt** for data transformation and modeling  
- **CSV datasets** as the raw data source  
---

## Project Structure

```plaintext
pipeline-airflow_dbt_snowflake-/
â”‚
â”œâ”€â”€ Snowflake_Scripts/
â”‚   â”œâ”€â”€ create_warehouse_and_db.sql
â”‚   â”œâ”€â”€ create_bronze_tables.sql
â”‚   â””â”€â”€ load_bronze_data.sql
â”‚
â”œâ”€â”€ dags/
â”‚   â”œâ”€â”€ dbt_dag.py
â”‚   â”œâ”€â”€ exampledag.py
â”‚   â”œâ”€â”€ .airflowignore
â”‚   â””â”€â”€ logs/
â”‚
â”œâ”€â”€ datasets/
â”‚   â”œâ”€â”€ CardBase.csv
â”‚   â”œâ”€â”€ CustomerBase.csv
â”‚   â”œâ”€â”€ FraudBase.csv
â”‚   â”œâ”€â”€ TransactionBase.csv
â”‚   â””â”€â”€ placeholder/
â”‚
â”œâ”€â”€ include/
â”‚   â””â”€â”€ profiles.yml
â”‚
â”œâ”€â”€ dbt_project/
â”‚   â”œâ”€â”€ macros/
â”‚   â”‚   â”œâ”€â”€ .gitkeep
â”‚   â”‚   â””â”€â”€ generate_schema_name.sql
â”‚   â”‚
â”‚   â”œâ”€â”€ snapshots/
â”‚   â”‚   â”œâ”€â”€ .gitkeep
â”‚   â”‚   â”œâ”€â”€ CardBase_snapshot.sql
â”‚   â”‚   â”œâ”€â”€ CustomerBase_snapshot.sql
â”‚   â”‚   â”œâ”€â”€ FraudBase_snapshots.sql
â”‚   â”‚   â””â”€â”€ TransactionBase_snapshots.sql
â”‚   â”‚
â”‚   â””â”€â”€ models/
â”‚
â”œâ”€â”€ tests/dags/
â”œâ”€â”€ logs/
â”œâ”€â”€ .dockerignore
â”œâ”€â”€ .gitignore
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ LICENSE
â””â”€â”€ README.md
```
## Data Flow

1. **Raw Data (CSV files)**  
   - Source data is collected in CSV format (`CardBase.csv`, `CustomerBase.csv`, `TransactionBase.csv`, `FraudBase.csv`).  
   - Files are stored in the `datasets/` folder.  

2. **Bronze Layer (Snowflake Stage & Tables)**  
   - A **Snowflake Stage** is created to load CSV files into the **bronze schema**.  
   - Data is ingested **as-is** from the CSVs into Snowflake tables:  
     - `bronze.CardBase`  
     - `bronze.CustomerBase`  
     - `bronze.TransactionBase`  
     - `bronze.FraudBase`  

3. **Silver Layer (dbt Transformations)**  
   - dbt models are built on top of bronze tables.  
   - Data cleaning and transformations are applied.  
   - **Incremental loading** is used â†’ only **new data** is processed (avoiding duplication).  
   - Outputs are stored in the **silver schema**.  

4. **Gold Layer (dbt Modeling)**  
   - Final business models are created for analytics.  
   - **Fact and Dimension tables** are designed for reporting.  
   - Stored in the **gold schema** for BI tools and analysis.  

<img width="914" height="456" alt="Screenshot 2025-09-29 141530" src="https://github.com/user-attachments/assets/d8bca132-bf04-4048-b987-cd00d8b5df93" />

---
## Orchestration (Airflow)
- **Airflow DAGs** are defined in `/dags/`.  
- They handle the sequence:
  1. Load raw data into Bronze (Snowflake)  
  2. Run dbt models for Silver layer  
  3. Build Gold layer (fact & dimension tables)  

---

## ðŸ“¬ Contact
- **Gmail**: drmohamedahmedb@gmail.com  
- **WhatsApp**: [+201127981884](https://wa.me/201127981884)  
- **LinkedIn**: [Ahmed Badawy](https://www.linkedin.com/in/ahmed-badawy-18275324b)  
- **GitHub**: [AhmedB5](https://github.com/AhmedB5)  





